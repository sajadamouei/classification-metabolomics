{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and functions\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from time import time\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.model_selection import (StratifiedKFold, cross_val_score, GridSearchCV,\n",
    "                                     train_test_split)\n",
    "from sklearn.metrics import (balanced_accuracy_score, classification_report,\n",
    "                             matthews_corrcoef, confusion_matrix, mean_squared_error,\n",
    "                             roc_curve, auc, roc_auc_score, accuracy_score,\n",
    "                             f1_score, precision_score)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "\n",
    "# Copy Datasets\n",
    "source_file_path = '/input/'\n",
    "destination_file_path = '/content/data.zip'\n",
    "shutil.copy(source_file_path, destination_file_path)\n",
    "\n",
    "# Uzip the zipped dataset file\n",
    "zip_file_path = '/content/data.zip'\n",
    "extract_to_path = '/content/data/'\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "############################################################################################# Plots\n",
    "# Function to plot histograms for different performance metrics\n",
    "def plot_histograms(auc_list, test_acc_list, mcc_list, bal_acc_list, model_name, method_name):\n",
    "    metrics = [auc_list, test_acc_list, mcc_list, bal_acc_list]\n",
    "    metric_names = [\"AUC\", \"Test Accuracy\", \"MCC\", \"Balanced Accuracy\"]\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # iterate over the metrics and plot histogram\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        plt.hist(metric, bins=20, edgecolor='black', alpha=0.7)\n",
    "        plt.title(metric_names[i])\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # filename and saving the figure\n",
    "    directory = \"/content/report/plots\"\n",
    "    filename = os.path.join(directory, f\"{model_name}_{method_name}.png\")\n",
    "\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Function to plot AUC with error bars for different models and methods\n",
    "def plot_auc_with_error_bars(results_df):\n",
    "    # Extracting relevant data for plotting\n",
    "    labels = results_df[\"Model\"] + \"-\" + results_df[\"Feature Reduction Method\"]\n",
    "    auc_means = results_df[\"AUC Mean\"].values\n",
    "    auc_stds = results_df[\"AUC Std\"].values\n",
    "\n",
    "    # Setting the figure size and creating the bar plot with error bars\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.bar(range(len(labels)), auc_means, yerr=auc_stds, align='center', alpha=0.7, ecolor='black', capsize=10)\n",
    "    plt.ylabel('AUC')\n",
    "    plt.xticks(range(len(labels)), labels, rotation=45, ha=\"right\")\n",
    "\n",
    "    # Annotations for better clarity\n",
    "    for i, v in enumerate(auc_means):\n",
    "        plt.text(i, v + 0.01, f\"{v:.2f}\", ha='center', va='bottom', fontsize=8, color='black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.title('AUC with Error Bars for Different ML Model-FR Method Combinations', y=1.05)\n",
    "\n",
    "    # Saving the plot\n",
    "    plt.savefig(\"/content/report/auc_barplot_with_error_bars.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_performance_heatmap(df, metric, save_path):\n",
    "\n",
    "  df[['Mean', 'CI']] = df[metric].str.split('±', expand=True)\n",
    "  df['Mean'] = df['Mean'].astype(float)\n",
    "\n",
    "  # Creating a pivot table for the heatmap\n",
    "  pivot_table = df.pivot(\"Model\", \"Feature Reduction Method\", \"Mean\")\n",
    "\n",
    "  # Plotting the heatmap\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  ax = sns.heatmap(pivot_table, annot=True, fmt=\".4f\", cmap=\"YlGnBu\")\n",
    "\n",
    "  # Annotation settings\n",
    "  fontsize_mean = 10 \n",
    "  fontsize_ci = 8\n",
    "  mean_color = 'white'\n",
    "  ci_color = 'darkorange'\n",
    "\n",
    "  # Adding the confidence interval and mean to the heatmap cells\n",
    "  for i, row in enumerate(pivot_table.values):\n",
    "      for j, _ in enumerate(row):\n",
    "          mean_value = pivot_table.iloc[i, j]\n",
    "          mean_text = f\"{mean_value:.4f}\"\n",
    "          ci_text = df.loc[(df['Model'] == pivot_table.index[i]) & (df['Feature Reduction Method'] == pivot_table.columns[j]), 'CI'].values[0]\n",
    "          \n",
    "          ax.text(j + 0.5, i + 0.7, f'±{ci_text}', horizontalalignment='center', verticalalignment='center', color=ci_color, fontsize=fontsize_ci)\n",
    "\n",
    "  plt.title(f'Heatmap of {metric} across Models and FR Methods')\n",
    "  plt.tight_layout()\n",
    "\n",
    "  # Save the heatmap\n",
    "  plt.savefig(save_path)\n",
    "  plt.close()\n",
    "\n",
    "# zip the contents of a folder\n",
    "def zip_folder(folder_path, output_filename):\n",
    "    shutil.make_archive(output_filename, 'zip', folder_path)\n",
    "\n",
    "    return f\"{output_filename}.zip\"\n",
    "\n",
    "# Configure pandas to display all rows in google colab\n",
    "pd.set_option('display.max_rows', None)\n",
    "# Ensure the directory for plots exists, create if it does not\n",
    "os.makedirs(\"/content/report/plots\", exist_ok=True)\n",
    "\n",
    "########################################################################################### Load dataset\n",
    "# Load dataset\n",
    "def load_dataset(sheet_name, remove_unknowns=True):\n",
    "    path = '/content/data/merge_m1_esi_plus_minus_norm.xlsx'\n",
    "    dataset = pd.read_excel(path, sheet_name=sheet_name, header=0)\n",
    "    dataset = dataset.sample(frac=1, random_state=666).reset_index(drop=True)\n",
    "    dataset = dataset.drop(columns=['eye', 'gender', 'age'])\n",
    "    # Optionally remove columns that start with 'unknown'\n",
    "    if remove_unknowns:\n",
    "        dataset = dataset.loc[:, ~dataset.columns.str.startswith('unknown')]\n",
    "    return dataset\n",
    "\n",
    "# Prepare the data for training\n",
    "def prepare_data(dataset):\n",
    "    # Extract labels for training\n",
    "    train_label = dataset['class']\n",
    "    # Drop the first and last columns from the dataset for training data\n",
    "    train_data = dataset.drop(columns=[dataset.columns[0], dataset.columns[-1]])\n",
    "    return train_data, train_label, dataset.columns.tolist()\n",
    "\n",
    "# Get the dataset and preprocess it based on feature selection\n",
    "def get_data(sheet_name, train_features):\n",
    "    if train_features == 'Only known metabolites':\n",
    "        dataset = load_dataset(sheet_name, remove_unknowns=True)\n",
    "    else:\n",
    "        dataset = load_dataset(sheet_name, remove_unknowns=False)\n",
    "    return prepare_data(dataset)\n",
    "\n",
    "# Select the data and feature types for the experiment\n",
    "data = \"M1 Compounds ESI+ and ESI- T.\" # @param [\"M1 Compounds ESI+ T.\", \"M1 Compounds ESI- T.\", \"M4 Compounds ESI+ T.\", \"M4 Compounds ESI- T.\", \"M1 Compounds ESI+ and ESI- T.\"]\n",
    "train_features = \"All metabolites\"\n",
    "\n",
    "# Retrieve the prepared data for training\n",
    "train_data, train_label, columns_list = get_data(data, train_features)\n",
    "\n",
    "# Convert the training data and labels to NumPy arrays of type 'float32'\n",
    "train_data = np.array(train_data).astype('float32')\n",
    "train_label = np.array(train_label).astype('float32')\n",
    "\n",
    "# Assign feature reduction method parameter\n",
    "fr_method = \"All methods\"\n",
    "\n",
    "# Assign machine learning model parameter\n",
    "ml_model = \"All models\"\n",
    "\n",
    "models_dict = {\n",
    "    \"XGBoost\": lambda: xgb.XGBClassifier(objective=\"binary:logistic\"),\n",
    "    \"Random Forest\": lambda: RandomForestClassifier(),\n",
    "    \"Support Vector Machine\": lambda: SVC(probability=True),\n",
    "    \"MLP\": lambda: MLPClassifier(),\n",
    "    \"SGD\": lambda: SGDClassifier(loss='log'),\n",
    "    \"Logistic Regression\": lambda: LogisticRegression(),\n",
    "    \"k-NN\": lambda: KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Dummy Classifier\": lambda: DummyClassifier(strategy='most_frequent')\n",
    "}\n",
    "\n",
    "param_grid_search = {\n",
    "    \"XGBoost\": {\n",
    "        'max_depth': [3, 4],  # default 3\n",
    "        'learning_rate': [0.1, 0.01],  # default 0.1\n",
    "        'n_estimators': [100, 150],  # default 100\n",
    "        'min_child_weight': [1, 2],  # default 1\n",
    "        'gamma': [0, 0.1],  # default 0\n",
    "        'subsample': [1.0],  # default 1\n",
    "        'colsample_bytree': [1.0],  # default 1\n",
    "    },\n",
    "\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': [100, 200],  # Default is 100\n",
    "        'criterion': ['gini', 'entropy'],  # Default is 'gini'\n",
    "        'max_depth': [None, 10],  # Default is None (full depth)\n",
    "        'min_samples_split': [2, 4],  # Default is 2\n",
    "        'min_samples_leaf': [1],  # Default is 1\n",
    "        'max_features': ['auto'],  # Default is 'auto' (sqrt)\n",
    "        'bootstrap': [True]  # Default is True\n",
    "    },\n",
    "    \"Support Vector Machine\": {\n",
    "        'C': [0.1, 1, 10],  # Default is 1.0\n",
    "        'gamma': ['scale', 'auto'],  # Default is 'scale'\n",
    "        'kernel': ['rbf']  # Fixed as rbf for this search\n",
    "    },\n",
    "    \"MLP\": {\n",
    "        'hidden_layer_sizes': [(100,), (50,50)],  # Default is (100,)\n",
    "        'activation': ['logistic', 'relu'],  # Default is 'relu'\n",
    "        'solver': ['adam', 'sgd'],  # Default is 'adam'\n",
    "        'alpha': [0.0001, 0.001],  # Default is 0.0001\n",
    "        'learning_rate': ['constant', 'adaptive'],  # Default is 'constant'\n",
    "        'max_iter': [200],  # Default is 200\n",
    "    },\n",
    "    \"SGD\": {\n",
    "        'loss': ['log'],  # default 'hinge'\n",
    "        'penalty': ['l2', 'l1', 'elasticnet'],  # default 'l2'\n",
    "        'alpha': [0.0001, 0.001],  # default 0.0001\n",
    "        'learning_rate': ['optimal', 'adaptive'],  # default 'optimal'\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        'penalty': ['l2'],  # default penalty\n",
    "        'C': [1.0, 0.1],  # default value is 1.0\n",
    "        'solver': ['lbfgs', 'newton-cg'],  # default solver is 'lbfgs'\n",
    "        'max_iter': [100],  # default is 100\n",
    "        'class_weight': [None, 'balanced']  # default is None\n",
    "    },\n",
    "    \"k-NN\": {\n",
    "        'n_neighbors': [3, 5],  # default is 5\n",
    "        'weights': ['uniform', 'distance'],  # default is 'uniform'\n",
    "        'algorithm': ['auto', 'ball_tree'],  # default is 'auto'\n",
    "        'leaf_size': [20, 30],  # default is 30\n",
    "        'p': [2]  # default is 2 (Euclidean distance)\n",
    "    },\n",
    "    \"Dummy Classifier\": {\n",
    "        \"strategy\": [\"most_frequent\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize results DataFrame\n",
    "results_df = pd.DataFrame(columns=[\"Model\", \"AUC\", \"AUC SD\", \"MCC\", \"MCC SD\",\"Precision\", \"Precision SD\", \"Sensitivity\", \"Sensitivity SD\", \"Specificity\", \"Specificity SD\", \"F1-score\", \"F1-score SD\", \"Balanced Accuracy\", \"Balanced Accuracy SD\", \"Test Accuracy\", \"Test Accuracy SD\", \"Train and test time\"])\n",
    "auc_raw_df = pd.DataFrame(columns=[\"Model\",\"AUC Scores\"])\n",
    "\n",
    "# Training and evaluating models function\n",
    "def train_and_evaluate(model_factory, x_data, y_data, model_name):\n",
    "\n",
    "    t0 = time()\n",
    "    # Lists to store metrics for each iteration\n",
    "    mcc_list, bal_acc_list, cv_acc_list, specificity_list, sensitivity_list, precision_list, f1_score_list, test_acc_list, roc_auc_list = [], [], [], [], [], [], [], [], []\n",
    "    auc_sc = None\n",
    "    model = model_factory()\n",
    "\n",
    "    # Set up stratified\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    # Conduct the k-fold cross-validation\n",
    "    for train_index, test_index in skf.split(x_data, y_data):\n",
    "        # Split the data into training and testing sets for the current fold\n",
    "        x_train_fold, x_test_fold = x_data[train_index], x_data[test_index]\n",
    "        y_train_fold, y_test_fold = y_data[train_index], y_data[test_index]\n",
    "\n",
    "        grid_search = GridSearchCV(estimator=model,\n",
    "                           param_grid=param_grid_search[model_name],\n",
    "                           cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=777),\n",
    "                           scoring='balanced_accuracy',\n",
    "                           verbose=1)\n",
    "\n",
    "        grid_search.fit(x_train_fold, y_train_fold)\n",
    "        new_model = grid_search.best_estimator_\n",
    "\n",
    "        # Predict on the test fold and calculate accuracy\n",
    "        if model_name == 'Support Vector Machine':\n",
    "          new_model.probability = True\n",
    "        y_pred = new_model.predict(x_test_fold)\n",
    "        test_acc_list.append(accuracy_score(y_test_fold, y_pred))\n",
    "\n",
    "        # Predict probabilities for the AUC calculation\n",
    "        y_score = new_model.predict_proba(x_test_fold)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test_fold, y_score)\n",
    "\n",
    "        # Calculate and store AUC, MCC, and balanced accuracy\n",
    "        roc_auc_list.append(auc(fpr, tpr))\n",
    "        mcc_list.append(matthews_corrcoef(y_test_fold, y_pred))\n",
    "        bal_acc_list.append(balanced_accuracy_score(y_test_fold, y_pred))\n",
    "\n",
    "        # Calculate precision, F1-score, specificity, and sensitivity\n",
    "        precision = precision_score(y_test_fold, y_pred)\n",
    "        f1_score_com = f1_score(y_test_fold, y_pred)\n",
    "        precision_list.append(precision)\n",
    "        f1_score_list.append(f1_score_com)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test_fold, y_pred).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        sensitivity = tp / (tp + fn)\n",
    "\n",
    "        specificity_list.append(specificity)\n",
    "        sensitivity_list.append(sensitivity)\n",
    "\n",
    "    # Compile all the collected metrics into a summary dictionary\n",
    "    auc_sc = {\n",
    "        \"Model\": model_name,\n",
    "        \"AUC Scores\":roc_auc_list\n",
    "          }\n",
    "    results = {\n",
    "        \"Model\": model_name,\n",
    "        \"AUC\": round(np.mean(roc_auc_list),4),\n",
    "        \"AUC SD\": round(np.std(roc_auc_list), 4),\n",
    "        \"MCC\": round(np.mean(mcc_list),4),\n",
    "        \"MCC SD\": round(np.std(mcc_list), 4),\n",
    "        \"Precision\": round(np.mean(precision_list),4),\n",
    "        \"Precision SD\": round(np.std(precision_list), 4),\n",
    "        \"Sensitivity\": round(np.mean(sensitivity_list),4),\n",
    "        \"Sensitivity SD\": round(np.std(sensitivity_list), 4),\n",
    "        \"Specificity\": round(np.mean(specificity_list),4),\n",
    "        \"Specificity SD\": round(np.std(specificity_list), 4),\n",
    "        \"F1-score\": round(np.mean(f1_score_list),4),\n",
    "        \"F1-score SD\": round(np.std(f1_score_list), 4),\n",
    "        \"Balanced Accuracy\": round(np.mean(bal_acc_list),4),\n",
    "        \"Balanced Accuracy SD\": round(np.std(bal_acc_list), 4),\n",
    "        \"Test Accuracy\": round(np.mean(test_acc_list),4),\n",
    "        \"Test Accuracy SD\": round(np.std(test_acc_list), 4),\n",
    "        \"Train and test time\": round(time()-t0, 2)\n",
    "    }\n",
    "    return results, auc_sc\n",
    "\n",
    "# Execute training and evaluation\n",
    "if ml_model == \"All models\":\n",
    "    # Loop through each model and method to train and evaluate\n",
    "    for model_name, model in tqdm(models_dict.items(), desc=\"Status\"):\n",
    "        # Train the model and collect results\n",
    "        result, auc_scroe = train_and_evaluate(model, train_data, train_label, model_name)\n",
    "        results_df = results_df.append(result, ignore_index=True)\n",
    "        auc_raw_df = auc_raw_df.append(auc_scroe, ignore_index=True)\n",
    "\n",
    "# Save the aggregated results to an Excel file for further analysis\n",
    "results_df.to_excel(\"/content/report/results.xlsx\", index=False)\n",
    "auc_raw_df.to_excel('/content/report/auc_scores_results.xlsx', index=False)\n",
    "\n",
    "# Visualize the performance of models using AUC with error bars\n",
    "#plot_auc_with_error_bars(results_df)\n",
    "\n",
    "# Plot and save the heatmap for AUC and Balanced acuuracy\n",
    "#plot_performance_heatmap(results_df, 'AUC', '/content/report/AUC_performance_heatmap.png')\n",
    "#plot_performance_heatmap(results_df, 'Balanced Accuracy', '/content/report/Balanced_Accuracy_performance_heatmap.png')\n",
    "#plot_performance_heatmap(results_df, 'MCC', '/content/report/MCC_performance_heatmap.png')\n",
    "\n",
    "# Package all results and plots into a downloadable zip file\n",
    "zip_path = zip_folder(\"/content/report\", \"/content/report\")\n",
    "# Print out the path to the zipped results\n",
    "print(f\"The results of this experiment, as well as the plots, are available for download in a zipped folder at the address: {zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
